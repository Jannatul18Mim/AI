CNN 
import tensorflow as tf 
from tensorflow.keras import datasets,layers,models 
import matplotlib.pyplot as plt 
import numpy as np 
(x_train, y_tarin),(x_test,y_test) = datasets.mnist.load_data() 
x_train.shape 
x_test.shape 
x_train,x_test= x_train/255.0 , x_test/255.0 
x_train= x_train.reshape(-1,28,28,1) 
x_test= x_test.reshape(-1,28,28,1) 
model= tf.keras.Sequential([ 
    layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)), 
    layers.MaxPooling2D((2,2)), 
    layers.Conv2D(64,(3,3),activation='relu'), 
    layers.MaxPooling2D((2,2)), 
    layers.Conv2D(64,(3,3),activation='relu'), 
    layers.Flatten(), 
    layers.Dense(64,activation='relu'), 
    layers.Dense(10,activation='softmax') 
]) 
model.summary() 
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy']) 
history= model.fit(x_train,y_tarin,epochs=5,validation_data=(x_test,y_test)) 
plt.plot(history.history['accuracy'],label='accuracy') 
plt.plot(history.history['val_accuracy'],label='val_accuracy') 
plt.xlabel('Epoch') 
plt.ylabel('Accuracy') 
plt.legend() 
plt.show() 
predictions= model.predict(x_test[:5]) 
for i in range(5): 
    plt.imshow(x_test[i].reshape(28,28,),cmap='gray') 
    plt.title(f"predicted:{tf.argmax(predictions[i])}, actual:{y_test[i]}") 
    plt.show() 
A* 
import heapq 
 
graph={ 
    'a':[('b',2),('e',3)], 
    'b':[('c',1),('g',9)], 
    'e':[('d',6)], 
    'c':[], 
    'd':[('g',1)], 
    'g':[] 
} 
 
heiuristic={ 
    'a':11, 
    'b':6, 
    'e':7, 
    'c':99, 
    'd':1, 
    'g':0, 
} 
 
def a_star(graph,start,goal,heiuristic): 
    pq=[(heiuristic[start],0,start,[start])] # (f, g, node, path) 
    visited=set() 
    while pq: 
        f_score,g_score,node,path=heapq.heappop(pq) 
        if node==goal: 
            return g_score,path 
        if node in visited: 
            continue 
        visited.add(node) 
        for neighbour,cost in graph.get(node,[]): 
            g=g_score+cost 
            f=g+heiuristic[neighbour] 
            heapq.heappush(pq,(f,g,neighbour,path+[neighbour])) 
    return None 
cost,path= a_star(graph,'a','g',heiuristic) 
print("cost: ",cost) 
print("path: ",path) 
 
greed best first 
import heapq 
 
graph={ 
    'a':[('b',2),('e',3)], 
    'b':[('c',1),('g',9)], 
    'e':[('d',6)], 
    'c':[], 
    'd':[('g',1)], 
    'g':[] 
} 
 
heiuristic={ 
    'a':11, 
    'b':6, 
    'e':7, 
    'c':99, 
    'd':1, 
    'g':0, 
} 
 
def greedy_best_first_search(graph,start,goal,heiuristic): 
    pq=[(heiuristic[start],start,[start])] 
    visited=set() 
    while pq: 
        h_score,node,path= heapq.heappop(pq) 
        if node==goal: 
            return path 
        if node in visited: 
            continue 
        visited.add(node) 
        for neighbour,cost in graph.get(node,[]): 
            if neighbour not in visited: 
                heapq.heappush(pq,(heiuristic[neighbour],neighbour,path+[neighbour])) 
    return None 
 
path= greedy_best_first_search(graph,'a','g',heiuristic) 
print("path: ",path) 
 
bfs 
graph={ 
    '5':['3','7'], 
    '3':['2','4'], 
    '7':['8'], 
    '2':[], 
    '4':['8'], 
    '8':[] 
} 
visited=[] 
queue=[] 
def bfs(visited,graph,root): 
    visited.append(root) 
    queue.append(root) 
    while queue: 
        m=queue.pop(0) 
        print(m,end="") 
        for neighbour in graph[m]: 
            if neighbour not in visited: 
                visited.append(neighbour) 
                queue.append(neighbour) 
bfs(visited,graph,'5')                 
 
dfs 
graph={ 
    'b':['d','e'], 
    'b':['d','e'], 
    'b':['d','e'], 
    'c':['f','g'], 
    'a':['b','c'], 
    'd':[], 
    'e':[], 
    'f':[], 
    'g':[] 
} 
 
def dfs(visited,graph,root): 
    if root not in visited: 
        visited.append(root) 
        for n in graph[root]: 
            dfs(visited,graph,n) 
    return visited 
visited=dfs([],graph,'a') 
print(visited)         
 
dfs limited 
graph={ 
    'a':[('b',2),('c',4)], 
    'b':[('d',7),('e',3)], 
    'c':[('f',5)], 
    'd':[], 
    'e':[('f',1)], 
    'f':[] 
} 
 
def depth_limited_dfs(graph,start,goal,limit): 
    def recursive_dls(node,path,depth): 
        if node==goal: 
            return path 
        if depth== limit: 
            return None 
        for neighbour, cost in graph.get(node, []): 
            if neighbour not in path: 
                result= recursive_dls(neighbour,path+[neighbour],depth+1) 
                if result is not None: 
                    return result 
        return None 
    return recursive_dls(start,[start],0) 
 
result= depth_limited_dfs(graph,'a','f',limit=1) 
print("result with depth=1: ",result) 
 
result= depth_limited_dfs(graph,'a','f',limit=2) 
print("result with depth=2: ",result) 
result= depth_limited_dfs(graph,'a','f',limit=4) 
print("result with depth=4: ",result) 
 
iterative dfs 
graph={ 
    'a':[('b',2),('c',4)], 
    'b':[('d',7),('e',3)], 
    'c':[('f',5)], 
    'd':[], 
    'e':[('f',1)], 
    'f':[] 
} 
 
def depth_limited_dfs(graph,start,goal,limit): 
    def recursive_dls(node,path,depth): 
        if node==goal: 
            return path 
        if depth== limit: 
            return None 
        for neighbour, cost in graph.get(node, []): 
            if neighbour not in path: 
                result= recursive_dls(neighbour,path+[neighbour],depth+1) 
                if result is not None: 
                    return result 
        return None 
    return recursive_dls(start,[start],0) 
 
def iterative_deepening_dls(graph,start,goal,max_depth): 
    for depth in range(max_depth+1): 
        print(f"searching with depth limit={depth}...") 
        result= depth_limited_dfs(graph,start,goal,depth) 
        if result is not None: 
            return result 
    return None 
 
result= iterative_deepening_dls(graph,'a','f',2) 
print("result path: ",result) 
 
ucs 
import heapq 
graph={ 
    'a':[('b',2),('c',4)], 
    'b':[('d',7),('e',3)], 
    'c':[('f',5)], 
    'd':[], 
    'e':[('f',1)], 
    'f':[] 
} 
def ucs(graph,start,goal): 
    pq=[(0,start,[start])] 
    visited=set() 
    while pq: 
        cost,node,path= heapq.heappop(pq) 
        if node  in visited: 
            continue 
        visited.add(node) 
        if node==goal: 
             return cost,path 
        for neighbour,edge_cost in graph.get(node,[]): 
            if neighbour not in visited: 
                heapq.heappush(pq,(cost+edge_cost,neighbour,path+[neighbour])) 
    return None 
cost,path= ucs(graph,'a','f') 
print("cheapest path cost= ",cost) 
print("path: ",path) 
 
biredectional 
import collections 
 
def bidirectional_bfs(graph, start, goal): 
    if start==goal: 
        return [start] 
    q1=collections.deque([start]) 
    q2=collections.deque([goal]) 
    p1={start:None} 
    p2={goal:None} 
    v1={start} 
    v2={goal} 
    meet=None 
    while q1 and q2: 
        u=q1.popleft() 
        for x in graph.get(u,[]): 
            if x not in v1: 
                v1.add(x) 
                p1[x]=u 
                q1.append(x) 
                if x in v2: 
                    meet=x 
                    q2.clear() 
                    break 
        if meet: 
            break 
        v=q2.popleft() 
        for y in graph.get(v,[]): 
            if y not in v2: 
                v2.add(y) 
                p2[y]=v 
                q2.append(y) 
                if y in v1: 
                    meet=y 
                    q1.clear() 
                    break 
        if meet: 
            break 
    if not meet: 
        return [] 
    left=[] 
    x=meet 
    while x is not None: 
        left.append(x) 
        x=p1[x] 
    left=list(reversed(left)) 
    right=[] 
    y=p2[meet] 
    while y is not None: 
        right.append(y) 
        y=p2[y] 
    return left+right 
 
if __name__=="__main__": 
    g={'A':['B'], 'B':['C'], 'C':['D'], 'D':['E'], 'E':[]} 
    print(bidirectional_bfs(g,'A','E')) 
 
n-queen hill climbing 
import random 
 
def conflicts(state): 
    n=len(state) 
    c=0 
    for i in range(n): 
        for j in range(i+1,n): 
            if state[i]==state[j] or abs(state[i]-state[j])==abs(i-j): 
                c+=1 
    return c 
 
def neighbors(state): 
    n=len(state) 
    for col in range(n): 
        for row in range(n): 
            if row!=state[col]: 
                t=list(state) 
                t[col]=row 
                yield tuple(t) 
 
def hill(n,max_steps): 
    cur=tuple(random.randrange(n) for _ in range(n)) 
    curc=conflicts(cur) 
    for _ in range(max_steps): 
        best=cur 
        bestc=curc 
        for nb in neighbors(cur): 
            c=conflicts(nb) 
            if c<bestc: 
                bestc=c 
                best=nb 
        if bestc<curc: 
            cur=best 
            curc=bestc 
            if curc==0: 
                break 
        else: 
            break 
    return cur,curc 
 
if __name__=="__main__": 
    print(hill(8,10000)) 
 
puzzle hill climbing 
def manhattan(s): 
    n = len(s) 
    pos = {} 
    for i in range(n): 
        for j in range(n): 
            v = goal[i][j] 
            pos[v] = (i, j) 
    d = 0 
    for i in range(n): 
        for j in range(n): 
            v = s[i][j] 
            if v != 0: 
                gi, gj = pos[v] 
                d += abs(gi-i)+abs(gj-j) 
    return d 
 
 
def neighbors(s): 
    n = len(s) 
    for i in range(n): 
        for j in range(n): 
            if s[i][j] == 0: 
                z = (i, j) 
    i, j = z 
    res = [] 
    for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]: 
        ni, nj = i+di, j+dj 
        if 0 <= ni < n and 0 <= nj < n: 
            t = [list(r) for r in s] 
            t[i][j], t[ni][nj] = t[ni][nj], t[i][j] 
            res.append(tuple(tuple(r) for r in t)) 
    return res 
 
 
def hill(initial, max_steps): 
    cur = initial 
    h = manhattan(cur) 
    for _ in range(max_steps): 
        best = cur 
        besth = h 
        for nb in neighbors(cur): 
            hb = manhattan(nb) 
            if hb < besth: 
                besth = hb 
                best = nb 
        if besth < h: 
            cur = best 
            h = besth 
        else: 
            break 
    return cur, h 
 
 
if __name__ == "__main__": 
    goal = ((1, 2, 3), (4, 5, 6), (7, 8, 0)) 
    initial = ((3, 8, 5), (6, 7, 1), (2, 0, 4)) 
    print(hill(initial, 100)) 
 
first choice 
import random 
 
def first_choice_hc(f,neighbors,init,max_steps): 
    x=init() 
    fx=f(x) 
    for _ in range(max_steps): 
        nbr=list(neighbors(x)) 
        random.shuffle(nbr) 
        improved=False 
        for y in nbr: 
            fy=f(y) 
            if fy<fx: 
                x=y 
                fx=fy 
                improved=True 
                break 
        if not improved: 
            break 
    return x,fx 
 
if __name__=="__main__": 
    def f(x): return (x-3)*(x-3) 
    def neighbors(x): return [x-1,x+1] 
    init=lambda: random.randint(-10,10) 
    print(first_choice_hc(f,neighbors,init,100)) 
 
hill climbing 
import random 
 
def hill_climb(f,neighbor,init,max_steps): 
    x=init() 
    fx=f(x) 
    for _ in range(max_steps): 
        best=fx 
        bestx=x 
        for n in neighbor(x): 
            fn=f(n) 
            if fn<best: 
                best=fn 
                bestx=n 
        if best<fx: 
            x=bestx 
            fx=best 
        else: 
            break 
    return x,fx 
 
if __name__=="__main__": 
    def f(x): return (x-3)*(x-3)+1 
    def neighbor(x): return [x-1,x+1] 
    init=lambda: random.randint(-10,10) 
    print(hill_climb(f,neighbor,init,50)) 
 
local beam 
import random 
 
def local_beam_search(f,neighbors,init,k,max_steps): 
    states=[init() for _ in range(k)] 
    values=[f(s) for s in states] 
    for _ in range(max_steps): 
        pairs=list(zip(values,states)) 
        pairs.sort() 
        bestv=pairs[0][0] 
        new_states=[] 
        for _,s in pairs[:k]: 
            for y in neighbors(s): 
                new_states.append(y) 
        if not new_states: 
            break 
        values=[f(s) for s in new_states] 
        pairs=list(zip(values,new_states)) 
        pairs.sort() 
        states=[s for _,s in pairs[:k]] 
        values=[v for v,_ in pairs[:k]] 
        if values[0]>=bestv: 
            break 
    return states[0],values[0] 
 
if __name__=="__main__": 
    def f(x): return (x-3)*(x-3) 
    def neighbors(x): return [x-1,x+1] 
    init=lambda: random.randint(-50,50) 
    print(local_beam_search(f,neighbors,init,3,100)) 
 
random restart 
import random 
 
def hill_climb(f,neighbors,init,max_steps): 
    x=init() 
    fx=f(x) 
    for _ in range(max_steps): 
        best=fx 
        bestx=x 
        for y in neighbors(x): 
            fy=f(y) 
            if fy<best: 
                best=fy 
                bestx=y 
        if best<fx: 
            x=bestx 
            fx=best 
        else: 
            break 
    return x,fx 
 
def random_restart(f,neighbors,init,max_steps,restarts): 
    bestx=None 
    bestv=None 
    for _ in range(restarts): 
        x,v=hill_climb(f,neighbors,init,max_steps) 
        if bestv is None or v<bestv: 
            bestv=v 
            bestx=x 
    return bestx,bestv 
 
if __name__=="__main__": 
    def f(x): return (x-3)*(x-3) 
    def neighbors(x): return [x-1,x+1] 
    init=lambda: random.randint(-50,50) 
    print(random_restart(f,neighbors,init,50,20)) 
 
simulated anneling 
import random,math 
 
def simulated_annealing(f,neighbors,init,max_steps,t0,alpha): 
    x=init() 
    fx=f(x) 
    T=t0 
    for _ in range(max_steps): 
        nbr=list(neighbors(x)) 
        if not nbr: 
            break 
        y=random.choice(nbr) 
        fy=f(y) 
        if fy<fx or random.random()<math.exp(-(fy-fx)/max(T,1e-9)): 
            x=y 
            fx=fy 
        T=T*alpha 
        if T<1e-12: 
            break 
    return x,fx 
 
if __name__=="__main__": 
    def f(x): return (x-3)*(x-3) 
    def neighbors(x): return [x-1,x+1] 
    init=lambda: random.randint(-50,50) 
    print(simulated_annealing(f,neighbors,init,1000,1.0,0.995)) 
 
stochastic hill climbing 
import random 
 
def stochastic_hc(f,neighbors,init,max_steps): 
    x=init() 
    fx=f(x) 
    for _ in range(max_steps): 
        nbr=list(neighbors(x)) 
        if not nbr: 
            break 
        y=random.choice(nbr) 
        fy=f(y) 
        if fy<fx: 
            x=y 
            fx=fy 
        else: 
            tried=1 
            improved=False 
            while tried<len(nbr): 
                y=random.choice(nbr) 
                fy=f(y) 
                if fy<fx: 
                    x=y 
                    fx=fy 
                    improved=True 
                    break 
                tried+=1 
            if not improved: 
                break 
    return x,fx 
 
if __name__=="__main__": 
    def f(x): return (x-3)*(x-3) 
    def neighbors(x): return [x-1,x+1] 
    init=lambda: random.randint(-10,10) 
    print(stochastic_hc(f,neighbors,init,100)) 
 
naïve bayer, decision tree previous  
import pandas as pd 
from sklearn.preprocessing import LabelEncoder 
from sklearn.naive_bayes import GaussianNB 
from sklearn.tree import DecisionTreeClassifier, plot_tree 
import matplotlib.pyplot as plt 
 
# Step 1: Create dataset 
data = { 
    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 
                'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 
                'overcast', 'rainy'], 
    'Temperature': ['hot', 'hot', 'hot', 'mild', 'cool', 'cool', 
                    'mild', 'cool', 'mild', 'mild', 'mild', 'hot', 
                    'mild', 'cool'], 
    'Humidity': ['high', 'high', 'high', 'high', 'normal', 'normal', 
                 'normal', 'high', 'normal', 'normal', 'normal', 'high', 
                 'normal', 'high'], 
    'Windy': ['false', 'true', 'false', 'false', 'false', 'true', 
              'true', 'false', 'false', 'false', 'true', 'true', 
              'false', 'true'], 
    'Play': ['no', 'no', 'yes', 'yes', 'yes', 'no', 
             'yes', 'no', 'yes', 'yes', 'yes', 'yes', 
             'yes', 'no'] 
} 
 
df = pd.DataFrame(data) 
 
# Step 2: Encode features and target 
le_outlook = LabelEncoder() 
le_temp = LabelEncoder() 
le_humid = LabelEncoder() 
le_windy = LabelEncoder() 
le_play = LabelEncoder() 
 
df['Outlook_n'] = le_outlook.fit_transform(df['Outlook']) 
df['Temperature_n'] = le_temp.fit_transform(df['Temperature']) 
df['Humidity_n'] = le_humid.fit_transform(df['Humidity']) 
df['Windy_n'] = le_windy.fit_transform(df['Windy']) 
df['Play_n'] = le_play.fit_transform(df['Play']) 
 
# Step 3: Prepare data 
X = df[['Outlook_n', 'Temperature_n', 'Humidity_n', 'Windy_n']] 
y = df['Play_n'] 
 
# Step 4: Train Naive Bayes 
nb_model = GaussianNB() 
nb_model.fit(X, y) 
 
# Step 5: Test new case — for example: sunny, cool, high, true 
sample = [[ 
    le_outlook.transform(['sunny'])[0], 
    le_temp.transform(['cool'])[0], 
    le_humid.transform(['high'])[0], 
    le_windy.transform(['true'])[0] 
]] 
 
nb_pred = nb_model.predict(sample) 
print("Naive Bayes Prediction:", le_play.inverse_transform(nb_pred)[0]) 
 
# Step 6: Train Decision Tree 
tree_model = DecisionTreeClassifier(criterion='entropy', random_state=0) 
tree_model.fit(X, y) 
 
tree_pred = tree_model.predict(sample) 
print("Decision Tree Prediction:", le_play.inverse_transform(tree_pred)[0]) 
 
# Step 7: Visualize the decision tree 
plt.figure(figsize=(10,6)) 
plot_tree(tree_model, 
          feature_names=['Outlook','Temperature','Humidity','Windy'], 
          class_names=['No','Yes'], filled=True) 
plt.show() 
 
naïve bayer 
# Naive Bayes Classifier Implementation with Detailed Posterior Calculation 
dataset = [ 
    ['Sunny', 'Hot', 'High', False, 'No'], 
    ['Sunny', 'Hot', 'High', True, 'No'], 
    ['Overcast', 'Hot', 'High', False, 'Yes'], 
    ['Rain', 'Mild', 'High', False, 'Yes'], 
    ['Rain', 'Cool', 'Normal', False, 'Yes'], 
    ['Rain', 'Cool', 'Normal', True, 'No'], 
    ['Overcast', 'Cool', 'Normal', True, 'Yes'], 
    ['Sunny', 'Mild', 'High', False, 'No'], 
    ['Sunny', 'Cool', 'Normal', False, 'Yes'], 
    ['Rain', 'Mild', 'Normal', False, 'Yes'], 
    ['Sunny', 'Mild', 'Normal', True, 'Yes'], 
    ['Overcast', 'Mild', 'High', True, 'Yes'], 
    ['Overcast', 'Hot', 'Normal', False, 'Yes'], 
    ['Rain', 'Mild', 'High', True, 'No'] 
] 
 
 
feature_names = ['Outlook', 'Temp', 'Humidity', 'Windy'] 
 
 
def train_naive_bayes(data, feature_names): 
    label_counts = {} 
    feature_counts = {} 
 
    for row in data: 
        features = row[:-1] 
        label = row[-1] 
 
   
        label_counts[label] = label_counts.get(label, 0) + 1 
 
        # Init dicts for label 
        if label not in feature_counts: 
            feature_counts[label] = {f: {} for f in feature_names} 
 
        for i, feature in enumerate(feature_names): 
            value = features[i] 
            feature_counts[label][feature][value] = feature_counts[label][feature].get(value, 0) + 1 
 
    return label_counts, feature_counts 
 
 
 
def predict_naive_bayes(x, label_counts, feature_counts): 
    total = sum(label_counts.values()) 
    probs = {} 
 
    print("\n--- Posterior Calculation ---") 
    for label in label_counts: 
        prior = label_counts[label] / total 
        probs[label] = prior 
        print(f"\nClass = {label}") 
        print(f"Prior P({label}) = {label_counts[label]}/{total} = {prior:.4f}") 
 
        for i, feature in enumerate(feature_names): 
            value = x[i] 
            feature_value_count = feature_counts[label][feature].get(value, 0) 
            unique_values = len(feature_counts[label][feature]) 
            smoothed_prob = (feature_value_count + 1) / (label_counts[label] + unique_values) 
 
            probs[label] *= smoothed_prob 
            print(f"P({feature}={value} | {label}) = 
({feature_value_count}+1)/({label_counts[label]}+{unique_values}) = {smoothed_prob:.4f}") 
 
        print(f"Posterior (unnormalized) for {label}: {probs[label]:.6f}") 
 
    prediction = max(probs, key=probs.get) 
    print("\nPredicted Class:", prediction) 
    return prediction 
 
 
 
label_counts, feature_counts = train_naive_bayes(dataset, feature_names) 
 
 
test_sample = ['Sunny', 'Cool', 'High', True]   
print("Test Sample:", test_sample) 
predict_naive_bayes(test_sample, label_counts, feature_counts) 
 
decision tree 
import math 
 
def entropy(y): 
    n=len(y) 
    from collections import Counter 
    c=Counter(y) 
    return -sum((v/n)*math.log2(v/n) for v in c.values()) 
 
def majority(y): 
    from collections import Counter 
    return Counter(y).most_common(1)[0][0] 
 
def info_gain(X,y,feat): 
    n=len(y) 
    vals={x[feat] for x in X} 
    e=entropy(y) 
    s=0 
    for v in vals: 
        idx=[i for i,x in enumerate(X) if x[feat]==v] 
        if not idx: 
            continue 
        py=[y[i] for i in idx] 
        s+=len(idx)/n*entropy(py) 
    return e-s 
 
def build(X,y,feats): 
    if len(set(y))==1: 
        return ('leaf',y[0]) 
    if not feats: 
        return ('leaf',majority(y)) 
    gains=[(info_gain(X,y,f),f) for f in feats] 
    gains.sort(reverse=True) 
    best=gains[0][1] 
    vals={x[best] for x in X} 
    node=('node',best,{}) 
    for v in vals: 
        idx=[i for i,x in enumerate(X) if x[best]==v] 
        if not idx: 
            node[2][v]=('leaf',majority(y)) 
        else: 
            Xv=[X[i][:best]+X[i][best+1:] for i in idx] 
            yv=[y[i] for i in idx] 
            fv=[f for f in feats if f!=best] 
            child=build(Xv,yv,[f-1 if f>best else f for f in fv]) 
            node[2][v]=child 
    return node 
 
def predict(tree,x): 
    t=tree 
    while t[0]=='node': 
        f=t[1] 
        v=x[f] 
        if v not in t[2]: 
            return None 
        x=x[:f]+x[f+1:] 
        t=t[2][v] 
    return t[1] 
 
if __name__=="__main__": 
    
X=[["sunny","hot","high","weak"],["sunny","hot","high","strong"],["overcast","hot","high","wea
 k"],["rain","mild","high","weak"],["rain","cool","normal","weak"],["rain","cool","normal","stron
 g"],["overcast","cool","normal","strong"],["sunny","mild","high","weak"],["sunny","cool","norm
 al","weak"],["rain","mild","normal","weak"],["sunny","mild","normal","strong"],["overcast","mil
 d","high","strong"],["overcast","hot","normal","weak"],["rain","mild","high","strong"]] 
    y=["no","no","yes","yes","yes","no","yes","no","yes","yes","yes","yes","yes","no"] 
    m={} 
    idc=0 
    Xenc=[] 
    for row in X: 
        r=[] 
        for v in row: 
            if v not in m: 
                m[v]=idc; idc+=1 
            r.append(m[v]) 
        Xenc.append(r) 
    tree=build(Xenc,y,list(range(len(Xenc[0])))) 
    x=["sunny","cool","high","strong"] 
    xenc=[m[v] for v in x] 
    print(predict(tree,xenc)) 
 
ngram 
import random 
class NGram: 
    def __init__(self,n): 
        self.n=n 
        self.counts={} 
        self.context_counts={} 
    def train(self,tokens): 
        pad=['<s>']*(self.n-1) 
        seq=pad+tokens+['</s>'] 
        for i in range(self.n-1,len(seq)): 
            ctx=tuple(seq[i-self.n+1:i]) 
            tok=seq[i] 
            self.counts[(ctx,tok)]=self.counts.get((ctx,tok),0)+1 
            self.context_counts[ctx]=self.context_counts.get(ctx,0)+1 
    def prob(self,ctx,tok): 
        ctx=tuple(ctx) 
        return (self.counts.get((ctx,tok),0)+1)/(self.context_counts.get(ctx,0)+len({t for (_,t) in 
self.counts.keys()})) 
    def generate(self,max_len=20): 
        ctx=['<s>']*(self.n-1) 
        out=[] 
        for _ in range(max_len): 
            ctuple=tuple(ctx) 
            vocab=list({t for (c,t) in self.counts.keys() if c==ctuple}) 
            if not vocab: 
                break 
            weights=[self.prob(ctx,v) for v in vocab] 
            s=sum(weights) 
            r=random.random()*s 
            acc=0 
            pick=vocab[-1] 
            for v,w in zip(vocab,weights): 
                acc+=w 
                if acc>=r: 
                    pick=v 
                    break 
            if pick=='</s>': 
                break 
            out.append(pick) 
            ctx=ctx[1:]+[pick] 
        return out 
if __name__=="__main__": 
    text="the cat sat on the mat the cat saw a rat".split() 
    lm=NGram(2) 
    lm.train(text) 
    print(lm.generate()) 
 
 
